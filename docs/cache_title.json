{"_default": {"1": {"path": "/README.md", "hash": "285dd5efeafddbb1af7e8ec373640a5f", "title": "CLIP Feature Extraction and Logistic Regression in CIFAR100"}, "2": {"path": "/README.md:1-17", "hash": "ce7641cb733a27067a2e4dee448117a6", "title": "CLIP: Zero-Shot Image Classification with Neural Networks"}, "3": {"path": "/README.md:19-55", "hash": "93d234755c7588256e844fb60af96221", "title": "Install and Load CLIP Model"}, "4": {"path": "/README.md:57-77", "hash": "49c072457024aec9f1a061518556fea4", "title": "CLIP Library: Tokenization and Visual-Textual Similarity"}, "5": {"path": "/README.md:79-106", "hash": "c0d3f800161500ccc71cba90c9150584", "title": "CLIP Model Zero-Shot Prediction with CIFAR-100 Image"}, "6": {"path": "/README.md:107-138", "hash": "c65da83edd11b3d89033383fa9559495", "title": "Image-Text Similarity Calculator"}, "7": {"path": "/README.md:141-177", "hash": "36d93e03e11ac40202273dcca4d5f0ce", "title": "CLIP-Based CIFAR100 Feature Extraction"}, "8": {"path": "/README.md:179-199", "hash": "f997aff2f30460252b12b469715115a5", "title": "Image Feature Logistic Regression"}, "9": {"path": "/clip/__init__.py", "hash": "a42ba491613c02268fbc26e1796d79f8", "title": "Importing Clip Module"}, "10": {"path": "/clip/clip.py", "hash": "656c63b1722dc730e0d5fb2ba3ef3b2f", "title": "CLIP Tokenizer Implementation"}, "11": {"path": "/clip/clip.py:1-33", "hash": "9afcb58c3a745ade617f726839b7c86a", "title": "Setting Up Model and Tokenizer"}, "12": {"path": "/clip/clip.py:33-43", "hash": "96460b7021b620076fca2715339c4c42", "title": "Download Pre-Trained Model URLs"}, "13": {"path": "/clip/clip.py:44-67", "hash": "11503a3660e007470a7ac6cf2b01d7d7", "title": "File Download and Verification Tool"}, "14": {"path": "/clip/clip.py:69-102", "hash": "5af9b8cc360fa9b33bc9d1b11ac1faff", "title": "SHA256 Checksum Verification and CLIP Operations"}, "15": {"path": "/clip/clip.py:103-131", "hash": "b92cf4eaad9613f8b9bde9742c06a766", "title": "CLIP Model Download and Loading"}, "16": {"path": "/clip/clip.py:132-157", "hash": "75c0aea1c7a550e644fa163117bf6cc7", "title": "JIT-Enabled Model Loading and Conversion"}, "17": {"path": "/clip/clip.py:158-184", "hash": "0bf4d89e8f3c0bc7d596730b8b77b317", "title": "Device-Specific Patching in Model Graph"}, "18": {"path": "/clip/clip.py:186-214", "hash": "916346e53bb6d0b50c26d157fd371aa3", "title": "Float Conversion and Contextual Tokenization"}, "19": {"path": "/clip/clip.py:215-237", "hash": "a1c0e36c53f762fc6c9522286e064d81", "title": "Encode Texts with Tokenizer"}, "20": {"path": "/clip/clip.py:238-245", "hash": "e5c50fd5b540be743f58df84bc463acc", "title": "Truncate and Store Tokens"}, "21": {"path": "/clip/model.py", "hash": "249e587be4b037132822eced2140d6ef", "title": "CLIP Model: Deep Learning with Attention Mechanisms"}, "22": {"path": "/clip/model.py:1-34", "hash": "34bd427e844989054b6d15e3d5cbc68d", "title": "Residual Bottleneck ConvBlock"}, "23": {"path": "/clip/model.py:35-61", "hash": "ffa704b6d99c4b52b99198e2b1bb42fc", "title": "Convolutional Block with Downsampling and Attention Pooling"}, "24": {"path": "/clip/model.py:62-83", "hash": "ad286a0ff5a191caa22642807c923a6f", "title": "Multi-Head Attention Implementation"}, "25": {"path": "/clip/model.py:84-109", "hash": "a4f2548022358ebde99ebb1c1dd1b678", "title": "Stem Convolutions and BatchNorm in ResNet"}, "26": {"path": "/clip/model.py:110-129", "hash": "ba6aba34f7391faaf68fc57374a2664f", "title": "ResNet Model with Attention Pooling"}, "27": {"path": "/clip/model.py:130-167", "hash": "bf537650717e041842bc2078f9ab6c8b", "title": "Bottleneck Blocks Model Initialization"}, "28": {"path": "/clip/model.py:168-196", "hash": "a5c30bbe8c953ce17753cfca7b699df8", "title": "Residual Attention Transformer Model"}, "29": {"path": "/clip/model.py:197-220", "hash": "e8984eba3aa320f5f08152631d2b8414", "title": "Vision Transformer Model Definition"}, "30": {"path": "/clip/model.py:221-248", "hash": "ae27cd55d3a9401a5a6113bf0d004350", "title": "CLIP Model: Convolutional-Transformer Feature Extractor"}, "31": {"path": "/clip/model.py:249-278", "hash": "6f46e1c73bf38a97c1df3af705cb86bc", "title": "Initializing Model Parameters"}, "32": {"path": "/clip/model.py:279-305", "hash": "e61b2b0ba7b809d9018eb02b0805bf51", "title": "Model Initialization and Layer Setup"}, "33": {"path": "/clip/model.py:306-322", "hash": "4af089aebb6df715c60f4d4e8b08fa17", "title": "Neural Network Weight Initialization"}, "34": {"path": "/clip/model.py:323-349", "hash": "bbea9eedb725668632c1c7bac92d64c5", "title": "Causal Attention Masked Transformer for CLIP"}, "35": {"path": "/clip/model.py:350-376", "hash": "1bcafbd107c679b0328183d9a15d69a0", "title": "Model.py Clip Creation Code"}, "36": {"path": "/clip/model.py:378-404", "hash": "ca35b556d26193055af3dfe47c8b2aac", "title": "Float16 Weight Conversion for Deep Learning Models"}, "37": {"path": "/clip/model.py:405-421", "hash": "a14139d8afbf903b5a5f30c5593d69b7", "title": "Vision Layer Count and Resolution Calculation"}, "38": {"path": "/clip/model.py:422-436", "hash": "ecfca86453071fbb9c06df1cf67b3b57", "title": "CLIP Model Initialization and Evaluation"}, "39": {"path": "/clip/simple_tokenizer.py", "hash": "74525bc862332bf5212e5bf42c74b011", "title": "Simple BPE Tokenizer Class"}, "40": {"path": "/clip/simple_tokenizer.py:1-30", "hash": "4fc6682b0010a5f560cf5d49af4196a3", "title": "Efficient BPE Encoding: Byte-to-Unicode Mapping"}, "41": {"path": "/clip/simple_tokenizer.py:31-68", "hash": "8f090c9d86aac5ea1722cd7d3823cd36", "title": "Byte Pair Encoding Tokenizer: Cleaning and Tokenization"}, "42": {"path": "/clip/simple_tokenizer.py:69-91", "hash": "9fd4e3e3dffb880d840661777341df4c", "title": "Tokenizer using Byte Pair Encoding"}, "43": {"path": "/clip/simple_tokenizer.py:92-124", "hash": "fe4c63eddeae0eabba7e893874668d37", "title": "Bigram Word Formation Algorithm"}, "44": {"path": "/clip/simple_tokenizer.py:125-132", "hash": "030b7861ee6a9a401ec957622c84aae5", "title": "Byte Pair Encoding Tokenizer"}, "45": {"path": "/data/country211.md", "hash": "9d001ba1fe882166588bf9c6e3090dba", "title": "Download and Extract Country211 Dataset"}, "46": {"path": "/data/rendered-sst2.md", "hash": "f9fd9cfbce3e75490f1b6e549df75b15", "title": "Rendered SST2 Dataset for Image Classification"}, "47": {"path": "/data/yfcc100m.md", "hash": "365818ed4d6f876fab287dfe4bf2ac41", "title": "YFCC100M Dataset Download and Decompression"}, "48": {"path": "/hubconf.py", "hash": "5e3e74a64070d69fbe20dfe939c196b1", "title": "CLIP Model Loading and Image Conversion"}, "49": {"path": "/hubconf.py:1-32", "hash": "cbe46420f8009a57f0e552a81701be80", "title": "Create Hub Entry Point for CLIP Models"}, "50": {"path": "/hubconf.py:33-42", "hash": "7ffd9cafde76d67fe19df2acb3ffc502", "title": "PIL to Tensor Conversion with Model Entrypoints"}, "51": {"path": "/model-card.md", "hash": "81cf53e46db790248cc597ed0e3bb3a6", "title": "Multimodal CLIP Model: Limitations and Feedback"}, "52": {"path": "/model-card.md:1-15", "hash": "e1fff4d195bd577b17848e0213644085", "title": "CLIP Model Card Overview"}, "53": {"path": "/model-card.md:15-36", "hash": "06f6d7dd1c94a51265d342dc09e64f80", "title": "CLIP Model: Vision Transformer or ResNet"}, "54": {"path": "/model-card.md:36-46", "hash": "11a07d2410567fef3abfbc8a5d8a926f", "title": "Model Purpose and Limitations"}, "55": {"path": "/model-card.md:46-56", "hash": "ec2cee3cc1d50980b9fe549b81502e1f", "title": "Task-Specific Testing for CLIP"}, "56": {"path": "/model-card.md:56-68", "hash": "5c60b7154ffe31acef23dbdf7a0a3a56", "title": "Crawled Data Performance Evaluation"}, "57": {"path": "/model-card.md:68-106", "hash": "70f1f2cb060690ba5608275eee4c9f29", "title": "Evaluating CLIP's Performance and Limitations"}, "58": {"path": "/model-card.md:108-112", "hash": "40f3ff6a616e95b44c6acbf9465334d0", "title": "CLIP's Biases and Gender Classification Accuracy"}, "59": {"path": "/model-card.md:112-120", "hash": "de30b835e0b8a18229e68cdc708290fe", "title": "Model Accuracy Evaluation & Risk Identification"}, "60": {"path": "/notebooks/Interacting_with_CLIP.py", "hash": "bebe19fab0123846fc8295c590dfa536", "title": "Visualizing CLIP Model's Text-Image Similarity in CIFAR-100"}, "61": {"path": "/notebooks/Interacting_with_CLIP.py:1-46", "hash": "def888b766f19442b429d18ea21c5899", "title": "CLIP Model Setup and Tokenization"}, "62": {"path": "/notebooks/Interacting_with_CLIP.py:47-80", "hash": "a8eed443345ef884da1d4577ee4fd0e9", "title": "Preparing CLIP Dataset from Images and Descriptions"}, "63": {"path": "/notebooks/Interacting_with_CLIP.py:81-110", "hash": "a41319457302e579cf331e944ec25d59", "title": "CLIP Feature Normalization and Cosine Similarity Heatmap"}, "64": {"path": "/notebooks/Interacting_with_CLIP.py:112-143", "hash": "7a5e0a5928b5f484316dff56d542adea", "title": "Text-Image Similarity Visualization using CIFAR-100 Dataset"}, "65": {"path": "/notebooks/Prompt_Engineering_for_ImageNet.py", "hash": "5b84fef3d412eb3594009811b64fce1d", "title": "ImageNet Classification via Prompt Engineering"}, "66": {"path": "/notebooks/Prompt_Engineering_for_ImageNet.py:1-36", "hash": "3f96b67a8a81e63529fc7e1920c2a95f", "title": "Install and Load ImageNet CLIP Dataset"}, "67": {"path": "/notebooks/Prompt_Engineering_for_ImageNet.py:38-59", "hash": "1ae06aa94b450807655ad87182aecc14", "title": "Zero-Shot ImageNet Classification with Code Snippet"}, "68": {"path": "/notebooks/Prompt_Engineering_for_ImageNet.py:60-84", "hash": "f607efc1a5a2199eaa35f14aadb85f23", "title": "Accuracy Calculation for ImageNet"}, "69": {"path": "/requirements.txt", "hash": "897a3b761244d669c9a573cf84fac1f5", "title": "Installing Essential Packages"}, "70": {"path": "/setup.py", "hash": "7ba30ecf418e983155357d414876b23d", "title": "Setting Up Python 'clip' Package"}, "71": {"path": "/tests/test_consistency.py", "hash": "e585fe041ddeb524dbb8cee84ff4f329", "title": "JIT vs Non-JIT CLIP Consistency Test"}}}
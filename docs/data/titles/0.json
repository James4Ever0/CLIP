{
    "/README.md": "CLIP Feature Extraction and Logistic Regression in CIFAR100",
    "/README.md:1-17": "CLIP: Zero-Shot Image Classification with Neural Networks",
    "/README.md:107-138": "Image-Text Similarity Calculator",
    "/README.md:141-177": "CLIP-Based CIFAR100 Feature Extraction",
    "/README.md:179-199": "Image Feature Logistic Regression",
    "/README.md:19-55": "Install and Load CLIP Model",
    "/README.md:57-77": "CLIP Library: Tokenization and Visual-Textual Similarity",
    "/README.md:79-106": "CLIP Model Zero-Shot Prediction with CIFAR-100 Image",
    "/clip/__init__.py": "Importing Clip Module",
    "/clip/clip.py": "CLIP Tokenizer Implementation",
    "/clip/clip.py:1-33": "Setting Up Model and Tokenizer",
    "/clip/clip.py:103-131": "CLIP Model Download and Loading",
    "/clip/clip.py:132-157": "JIT-Enabled Model Loading and Conversion",
    "/clip/clip.py:158-184": "Device-Specific Patching in Model Graph",
    "/clip/clip.py:186-214": "Float Conversion and Contextual Tokenization",
    "/clip/clip.py:215-237": "Encode Texts with Tokenizer",
    "/clip/clip.py:238-245": "Truncate and Store Tokens",
    "/clip/clip.py:33-43": "Download Pre-Trained Model URLs",
    "/clip/clip.py:44-67": "File Download and Verification Tool",
    "/clip/clip.py:69-102": "SHA256 Checksum Verification and CLIP Operations",
    "/clip/model.py": "CLIP Model: Deep Learning with Attention Mechanisms",
    "/clip/model.py:1-34": "Residual Bottleneck ConvBlock",
    "/clip/model.py:110-129": "ResNet Model with Attention Pooling",
    "/clip/model.py:130-167": "Bottleneck Blocks Model Initialization",
    "/clip/model.py:168-196": "Residual Attention Transformer Model",
    "/clip/model.py:197-220": "Vision Transformer Model Definition",
    "/clip/model.py:221-248": "CLIP Model: Convolutional-Transformer Feature Extractor",
    "/clip/model.py:249-278": "Initializing Model Parameters",
    "/clip/model.py:279-305": "Model Initialization and Layer Setup",
    "/clip/model.py:306-322": "Neural Network Weight Initialization",
    "/clip/model.py:323-349": "Causal Attention Masked Transformer for CLIP",
    "/clip/model.py:35-61": "Convolutional Block with Downsampling and Attention Pooling",
    "/clip/model.py:350-376": "Model.py Clip Creation Code",
    "/clip/model.py:378-404": "Float16 Weight Conversion for Deep Learning Models",
    "/clip/model.py:405-421": "Vision Layer Count and Resolution Calculation",
    "/clip/model.py:422-436": "CLIP Model Initialization and Evaluation",
    "/clip/model.py:62-83": "Multi-Head Attention Implementation",
    "/clip/model.py:84-109": "Stem Convolutions and BatchNorm in ResNet",
    "/clip/simple_tokenizer.py": "Simple BPE Tokenizer Class",
    "/clip/simple_tokenizer.py:1-30": "Efficient BPE Encoding: Byte-to-Unicode Mapping",
    "/clip/simple_tokenizer.py:125-132": "Byte Pair Encoding Tokenizer",
    "/clip/simple_tokenizer.py:31-68": "Byte Pair Encoding Tokenizer: Cleaning and Tokenization",
    "/clip/simple_tokenizer.py:69-91": "Tokenizer using Byte Pair Encoding",
    "/clip/simple_tokenizer.py:92-124": "Bigram Word Formation Algorithm",
    "/data/country211.md": "Download and Extract Country211 Dataset",
    "/data/rendered-sst2.md": "Rendered SST2 Dataset for Image Classification",
    "/data/yfcc100m.md": "YFCC100M Dataset Download and Decompression",
    "/hubconf.py": "CLIP Model Loading and Image Conversion",
    "/hubconf.py:1-32": "Create Hub Entry Point for CLIP Models",
    "/hubconf.py:33-42": "PIL to Tensor Conversion with Model Entrypoints",
    "/model-card.md": "Multimodal CLIP Model: Limitations and Feedback",
    "/model-card.md:1-15": "CLIP Model Card Overview",
    "/model-card.md:108-112": "CLIP's Biases and Gender Classification Accuracy",
    "/model-card.md:112-120": "Model Accuracy Evaluation & Risk Identification",
    "/model-card.md:15-36": "CLIP Model: Vision Transformer or ResNet",
    "/model-card.md:36-46": "Model Purpose and Limitations",
    "/model-card.md:46-56": "Task-Specific Testing for CLIP",
    "/model-card.md:56-68": "Crawled Data Performance Evaluation",
    "/model-card.md:68-106": "Evaluating CLIP's Performance and Limitations",
    "/notebooks/Interacting_with_CLIP.py": "Visualizing CLIP Model's Text-Image Similarity in CIFAR-100",
    "/notebooks/Interacting_with_CLIP.py:1-46": "CLIP Model Setup and Tokenization",
    "/notebooks/Interacting_with_CLIP.py:112-143": "Text-Image Similarity Visualization using CIFAR-100 Dataset",
    "/notebooks/Interacting_with_CLIP.py:47-80": "Preparing CLIP Dataset from Images and Descriptions",
    "/notebooks/Interacting_with_CLIP.py:81-110": "CLIP Feature Normalization and Cosine Similarity Heatmap",
    "/notebooks/Prompt_Engineering_for_ImageNet.py": "ImageNet Classification via Prompt Engineering",
    "/notebooks/Prompt_Engineering_for_ImageNet.py:1-36": "Install and Load ImageNet CLIP Dataset",
    "/notebooks/Prompt_Engineering_for_ImageNet.py:38-59": "Zero-Shot ImageNet Classification with Code Snippet",
    "/notebooks/Prompt_Engineering_for_ImageNet.py:60-84": "Accuracy Calculation for ImageNet",
    "/requirements.txt": "Installing Essential Packages",
    "/setup.py": "Setting Up Python 'clip' Package",
    "/tests/test_consistency.py": "JIT vs Non-JIT CLIP Consistency Test"
}